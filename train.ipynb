{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# data_dir = 'Synthetic_total/'\n",
    "data_dir = 'Please enter the dataset root path here'\n",
    "\n",
    "excel_file = 'Please enter the dataset file here'\n",
    "train = pd.read_csv(os.path.join(data_dir,excel_file))\n",
    "train.rename(columns={'ID': 'patient_id', 'REAL_STONE':'target'}, inplace=True)\n",
    "columns = ['HR', 'BT', 'AGE', 'DUCT_DILIATATION_10MM', 'Hb','PLT','WBC','ALP', 'ALT', 'AST', 'TOTAL_BILIRUBIN',  'target']\n",
    "train = train[columns]\n",
    "\n",
    "print(train['target'].value_counts())\n",
    "print(train.isna().sum())  # 컬럼별 NaN 개수 확인\n",
    "print(len(train))  # 전체 데이터셋에 NaN이 하나라도 있는지 확인\n",
    "print(len(train.dropna()))  # 전체 데이터셋에 NaN이 하나라도 있는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_dir = 'Please enter the dataset root path here'\n",
    "excel_file = 'Please enter the dataset file path here'\n",
    "\n",
    "data = pd.read_csv(os.path.join(data_dir, excel_file))\n",
    "\n",
    "train_df, test_data = train_test_split(data, test_size=0.3, stratify=data['target'], random_state=123, shuffle=True)\n",
    "val_df, test = train_test_split(test_data, test_size=0.4, stratify=test_data['target'], random_state=123,shuffle=True)\n",
    "test_origin = test.copy()\n",
    "\n",
    "train_df.drop(columns=['patient_id'],inplace = True)\n",
    "val_df.drop(columns=['patient_id'],inplace = True)\n",
    "test.drop(columns=['patient_id'],inplace = True)\n",
    "\n",
    "train = pd.concat([train, train_df, val_df],axis=0)\n",
    "train.reset_index(inplace=True, drop=True)\n",
    "test.reset_index(inplace=True, drop=True)\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pycaret.classification import *\n",
    "\n",
    "exp_clf = setup(data = train, target = 'target', session_id=11, n_jobs=5)\n",
    "best = compare_models(sort='F1', fold=10, n_select=5)\n",
    "tuned_model = [tune_model(i, choose_better=True) for i in best]\n",
    "blended_model = blend_models(estimator_list = tuned_model)\n",
    "stack_model = stack_models(estimator_list = tuned_model, fold = 5)\n",
    "final_model = finalize_model(stack_model)\n",
    "\n",
    "evaluate_model(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predict_model(tuned_model[0], data = test)\n",
    "\n",
    "df = prediction\n",
    "\n",
    "categories = ['ERCP', 'Stone']\n",
    "\n",
    "for category in categories:\n",
    "    column = 'prediction_label'\n",
    "    \n",
    "    if category == 'ERCP':\n",
    "        yes_count = (df[column] == 1).sum()\n",
    "        total_count = len(df)\n",
    "    else:  # Stone\n",
    "        yes_count = (df[df[column] == 1]['target'] == 1).sum()\n",
    "        total_count = (df[df[column] == 1]['target'] == 1).sum() + (df[df[column] == 1]['target'] == 0).sum()\n",
    "\n",
    "    yes_percent = round(yes_count / total_count * 100, 2)\n",
    "    no_count = total_count - yes_count\n",
    "    no_percent = round(100 - yes_percent, 2)\n",
    "    ercp_no_stone_yes = ((df[column] == 0) & (df['target'] == 1)).sum()\n",
    "    total_ercp_no = (df[column] == 0).sum()\n",
    "    percent = round(ercp_no_stone_yes / total_ercp_no * 100, 2)\n",
    "\n",
    "    print(f'{category} Yes: {yes_count} ({yes_percent}%)')\n",
    "    print(f'{category} No: {no_count} ({no_percent}%)')\n",
    "    print(f'ERCP No, REAL_STONE Yes: {ercp_no_stone_yes} ({percent}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# from pytz import timezone\n",
    "# from datetime import datetime\n",
    "\n",
    "# log_dir = 'logs/'\n",
    "# mode = 'test'\n",
    "# modality = 'tabular'\n",
    "# seoul_timezone = timezone('Asia/Seoul')\n",
    "# today_seoul = datetime.now(seoul_timezone)\n",
    "\n",
    "# directory_name = today_seoul.strftime(\"%Y-%m-%d-%H-%M\")\n",
    "\n",
    "# log_dir = log_dir+directory_name + '-' + mode + '-' + modality\n",
    "\n",
    "# if os.path.exists(log_dir):\n",
    "#     pass\n",
    "# else:\n",
    "#     os.makedirs(log_dir)\n",
    "    \n",
    "    \n",
    "# save_model(final_model, os.path.join(log_dir,'ensemble'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, auc, roc_curve\n",
    "import numpy as np\n",
    "\n",
    "def cal(df):\n",
    "    # AUROC\n",
    "    auroc = roc_auc_score(df[\"target\"], df[\"prediction_prob\"])\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(df[\"target\"], df[\"prediction_label\"])\n",
    "\n",
    "    # Confusion Matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(df[\"target\"], df[\"prediction_label\"]).ravel()\n",
    "\n",
    "    # Sensitivity (Recall)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "\n",
    "    # Specificity\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    # PPV (Positive Predictive Value, Precision)\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "\n",
    "    # NPV (Negative Predictive Value)\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "\n",
    "   \n",
    "    # 결과 출력\n",
    "    print(f\"AUROC: {auroc:.2f}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Sensitivity, Recall: {sensitivity:.2f}\")\n",
    "    print(f\"Specificity: {specificity:.2f}\")\n",
    "    print(f\"Precision, PPV: {ppv:.2f}\")\n",
    "    print(f\"NPV: {npv:.2f}\")\n",
    "    \n",
    "prediction = predict_model(final_model, data = test)\n",
    "\n",
    "# target 1에 대한 확률로 바꾸기\n",
    "prediction['prediction_prob'] = np.where(prediction['prediction_label'] == 1, prediction['prediction_score'], 1 - prediction['prediction_score'])    \n",
    "cal(prediction)\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "prediction.to_csv('DUMC_analysis/ExtraTreesClassifier.csv', index=False)\n",
    "# prediction.to_csv('DUMC_analysis/CatBoostClassifier_external.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction = predict_model(tuned_model[0], data = test)\n",
    "\n",
    "interpret_model(tuned_model[0], save = True)\n",
    "plot_model(tuned_model[0], plot = 'feature')\n",
    "# interpret_model(tuned_model[0], plot = 'correlation')\n",
    "# interpret_model(tuned_model[0], plot = 'msa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_score, accuracy_score, confusion_matrix\n",
    "from numpy import *\n",
    "\n",
    "# Seaborn 스타일 설정\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "def compute_youden_index_cutoff(y_true, y_prob):\n",
    "    \"\"\"\n",
    "    Calculate the optimal cutoff using Youden Index (J).\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "    J = tpr - fpr\n",
    "    # ix = argmax(J)\n",
    "    ix = np.argsort(J)[::-1][4]\n",
    "    \n",
    "    best_threshold = thresholds[ix]\n",
    "    sensitivity = tpr[ix]\n",
    "    specificity = 1 - fpr[ix]\n",
    "    y_pred = (y_prob >= best_threshold).astype(int)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    \n",
    "    return best_threshold, sensitivity, specificity, precision, fpr, tpr, thresholds, ix, y_pred\n",
    "\n",
    "def bootstrap_roc(y_true, y_prob, n_bootstraps=1000, random_seed=42):\n",
    "    \"\"\"\n",
    "    Calculate the ROC confidence intervals using bootstrapping.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_seed)\n",
    "    bootstrapped_tprs = []\n",
    "    base_fpr = np.linspace(0, 1, 100)\n",
    "    \n",
    "    for _ in range(n_bootstraps):\n",
    "        indices = rng.choice(len(y_true), size=len(y_true), replace=True)\n",
    "        y_true_boot = y_true[indices]\n",
    "        y_prob_boot = y_prob[indices]\n",
    "        fpr, tpr, _ = roc_curve(y_true_boot, y_prob_boot)\n",
    "        tpr_interp = np.interp(base_fpr, fpr, tpr)\n",
    "        tpr_interp[0] = 0.0\n",
    "        bootstrapped_tprs.append(tpr_interp)\n",
    "\n",
    "    bootstrapped_tprs = np.array(bootstrapped_tprs)\n",
    "    mean_tpr = bootstrapped_tprs.mean(axis=0)\n",
    "    std_tpr = bootstrapped_tprs.std(axis=0)\n",
    "    tpr_lower = np.maximum(mean_tpr - 1.96 * std_tpr, 0)\n",
    "    tpr_upper = np.minimum(mean_tpr + 1.96 * std_tpr, 1)\n",
    "    \n",
    "    return base_fpr, mean_tpr, tpr_lower, tpr_upper\n",
    "\n",
    "# Assuming y_true and y_prob are your data inputs\n",
    "y_true = prediction['target'].values\n",
    "y_prob = prediction['prediction_prob'].values\n",
    "\n",
    "# Calculate the optimal cutoff using Youden Index\n",
    "best_threshold, sensitivity, specificity, precision, fpr, tpr, thresholds, optimal_idx, y_pred = compute_youden_index_cutoff(y_true, y_prob)\n",
    "\n",
    "# Calculate AUC\n",
    "roc_auc = roc_auc_score(y_true, y_prob)\n",
    "\n",
    "# Bootstrap for 95% CI\n",
    "base_fpr, mean_tpr, tpr_lower, tpr_upper = bootstrap_roc(y_true, y_prob)\n",
    "\n",
    "# Calculate additional metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Confusion matrix for NPV calculation\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "# Specific metrics\n",
    "sensitivity = tp / (tp + fn)  # Recall (True Positive Rate)\n",
    "specificity = tn / (tn + fp)  # True Negative Rate\n",
    "precision = tp / (tp + fp)    # Positive Predictive Value\n",
    "npv = tn / (tn + fn)          # Negative Predictive Value\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot the mean ROC curve\n",
    "sns.lineplot(x=base_fpr, y=mean_tpr, color=\"skyblue\", label=\"Mean ROC Curve (AUC = %.2f)\" % roc_auc)\n",
    "\n",
    "# Plot the 95% confidence interval\n",
    "plt.fill_between(base_fpr, tpr_lower, tpr_upper, color=\"blue\", alpha=0.2, label=\"95% Confidence Interval\")\n",
    "\n",
    "# Diagonal line\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "\n",
    "# Highlight the optimal cutoff point\n",
    "plt.scatter(fpr[optimal_idx], tpr[optimal_idx], marker='+', s=100, color='red', \n",
    "            label='Optimal Cutoff = %.3f\\nSensitivity = %.3f\\nSpecificity = %.3f' % \n",
    "                  (best_threshold, sensitivity, specificity))\n",
    "\n",
    "# Axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve with 95% Confidence Interval')\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print metrics for the optimal cutoff\n",
    "print(f\"Optimal Cutoff Using Youden Index = {best_threshold:.3f}\")\n",
    "print(f\"AUROC: {roc_auc:.3f}\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.3f}\")\n",
    "print(f\"Specificity: {specificity:.3f}\")\n",
    "print(f\"Precision (PPV): {precision:.3f}\")\n",
    "print(f\"Negative Predictive Value (NPV): {npv:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, average_precision_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "plt.rcParams[\"font.family\"] = \"DejaVu Sans\"\n",
    "\n",
    "def decision_curve_analysis(y_true, y_prob, thresholds=None):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.arange(0.1, 0.7, 0.1)\n",
    "\n",
    "    net_benefit_model = []\n",
    "    net_benefit_all = []\n",
    "    net_benefit_none = 0  # Net benefit of treat-none is always 0\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        treat_model = (y_prob >= threshold).astype(int)\n",
    "        tp_model = np.sum((y_true == 1) & (treat_model == 1))\n",
    "        fp_model = np.sum((y_true == 0) & (treat_model == 1))\n",
    "        prob_tp = tp_model / len(y_true)\n",
    "        prob_fp = fp_model / len(y_true)\n",
    "        net_benefit = prob_tp - (prob_fp * threshold / (1 - threshold))\n",
    "\n",
    "        if net_benefit >= 0:\n",
    "            net_benefit_model.append(net_benefit)\n",
    "            net_benefit_all.append((np.sum(y_true == 1) / len(y_true)) - (np.sum(y_true == 0) / len(y_true)) * (threshold / (1 - threshold)))\n",
    "        else:\n",
    "            net_benefit_model.append(None)\n",
    "            net_benefit_all.append(None)\n",
    "\n",
    "    return {\n",
    "        'thresholds': thresholds,\n",
    "        'net_benefit_model': net_benefit_model,\n",
    "        'net_benefit_all': net_benefit_all,\n",
    "        'net_benefit_none': [net_benefit_none] * len(thresholds)\n",
    "    }\n",
    "\n",
    "def plot_roc_curve(ax, models_data):\n",
    "    for model_data in models_data:\n",
    "        fpr, tpr, _ = roc_curve(model_data['y_true'], model_data['y_prob'])\n",
    "        roc_auc = roc_auc_score(model_data['y_true'], model_data['y_prob'])\n",
    "        ax.plot(fpr, tpr, label=f'{model_data[\"label\"]} (AUC = {roc_auc:.2f})', color=model_data['color'])\n",
    "    ax.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Diagonal line (random model)\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('ROC Curve')\n",
    "    ax.legend(loc='lower right')\n",
    "\n",
    "def plot_dca(ax, models_data):\n",
    "    for model_data in models_data:\n",
    "        dca_results = decision_curve_analysis(model_data['y_true'], model_data['y_prob'])\n",
    "        filtered_thresholds = [t for t, nb in zip(dca_results['thresholds'], dca_results['net_benefit_model']) if nb is not None]\n",
    "        filtered_net_benefit_model = [nb for nb in dca_results['net_benefit_model'] if nb is not None]\n",
    "        filtered_net_benefit_all = [nb for nb in dca_results['net_benefit_all'] if nb is not None]\n",
    "        \n",
    "        ax.plot(filtered_thresholds, filtered_net_benefit_model, label=f'{model_data[\"label\"]}', color=model_data['color'])\n",
    "    ax.plot(filtered_thresholds, filtered_net_benefit_all, linestyle='--', color='green', label='Treat All')\n",
    "    ax.axhline(0, color='red', linestyle='--', label='Treat None')\n",
    "    ax.set_xlabel('Threshold Probability')\n",
    "    ax.set_ylabel('Net Benefit')\n",
    "    ax.set_title('Decision Curve Analysis (DCA)')\n",
    "    ax.legend(loc='lower right')\n",
    "\n",
    "def plot_pr_curve(ax, models_data):\n",
    "    for model_data in models_data:\n",
    "        precision, recall, _ = precision_recall_curve(model_data['y_true'], model_data['y_prob'])\n",
    "        ap = average_precision_score(model_data['y_true'], model_data['y_prob'])\n",
    "        ax.plot(recall, precision, label=f'{model_data[\"label\"]} (AP = {ap:.2f})', color=model_data['color'])\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_title('Precision-Recall (PR) Curve')\n",
    "    ax.legend(loc='lower left')\n",
    "\n",
    "def plot_all(csv_list):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "    models_data = []\n",
    "    for idx, csv_file in enumerate(csv_list):\n",
    "        data = pd.read_csv(csv_file)\n",
    "        y_true = data['target']\n",
    "        data['prediction_prob'] = np.where(data['prediction_label'] == 1, data['prediction_score'], 1 - data['prediction_score'])    \n",
    "        y_prob = data['prediction_prob']\n",
    "        experiment_name = csv_file.split(\"/\")[-1].split(\".\")[0]\n",
    "        models_data.append({'y_true': y_true, 'y_prob': y_prob, 'label': experiment_name, 'color': plt.cm.viridis(idx / len(csv_list))})\n",
    "\n",
    "    # Plot ROC, DCA, and PR Curve\n",
    "    plot_roc_curve(axs[0], models_data)\n",
    "    plot_dca(axs[1], models_data)\n",
    "    plot_pr_curve(axs[2], models_data)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"model_performance_comparison.png\", dpi=400)\n",
    "    plt.show()\n",
    "\n",
    "# 사용 예시\n",
    "csv_list = [\n",
    "    # 'DUMC_analysis/ExtraTreesClassifier.csv',\n",
    "    # 'DUMC_analysis/XGBClassifier.csv',\n",
    "    # 'DUMC_analysis/RandomForestClassifier.csv',\n",
    "    # 'DUMC_analysis/CatBoostClassifier.csv',\n",
    "    # 'DUMC_analysis/LGBMClassifier.csv',\n",
    "    \n",
    "    # 'DUMC_analysis/ExtraTreesClassifier_external.csv',\n",
    "    # 'DUMC_analysis/CatBoostClassifier_external.csv',  \n",
    "    # 'DUMC_analysis/XGBClassifier_external.csv',\n",
    "    # 'DUMC_analysis/RandomForestClassifier_external.csv',\n",
    "    # 'DUMC_analysis/LGBMClassifier_external.csv',\n",
    "]\n",
    "\n",
    "plot_all(csv_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd \n",
    "\n",
    "# Seaborn 스타일 적용\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "def plot_calibration_curve(ax, models_data):\n",
    "    for model_data in models_data:\n",
    "        # 보정 곡선 데이터 생성\n",
    "        fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "            model_data['y_true'], model_data['y_prob'], n_bins=10\n",
    "        )\n",
    "\n",
    "        \n",
    "        # 보정 곡선 그리기\n",
    "        # ax.scatter(mean_predicted_value, fraction_of_positives, \n",
    "        #         color=model_data['color'], marker='o', label=f'{model_data[\"label\"]}')\n",
    "        ax.scatter(mean_predicted_value, fraction_of_positives, \n",
    "                color=model_data['color'], marker='o')\n",
    "    \n",
    "    # 이상적인 보정선\n",
    "    ax.plot([0, 1], [0, 1], linestyle='--', color='black', lw=1.5, label='Ideal Calibration')\n",
    "    \n",
    "    # 그래프 레이블 및 제목 설정\n",
    "    ax.set_xlabel('Mean Predicted Probability')\n",
    "    ax.set_ylabel('Fraction of Positives')\n",
    "    ax.set_title('Calibration Curve')\n",
    "    ax.legend(loc='best', fontsize=10)\n",
    "\n",
    "def plot_all(csv_list):\n",
    "    # 플롯 설정\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(8, 6))  # 단일 보정 곡선 그래프\n",
    "    models_data = []\n",
    "    \n",
    "    # 데이터 처리 및 모델 데이터 준비\n",
    "    for idx, csv_file in enumerate(csv_list):\n",
    "        data = pd.read_csv(csv_file)\n",
    "        y_true = data['target']\n",
    "        \n",
    "        # target이 1일 확률로 변환\n",
    "        data['prediction_prob'] = np.where(data['prediction_label'] == 1, \n",
    "                                           data['prediction_score'], \n",
    "                                           1 - data['prediction_score'])    \n",
    "        y_prob = data['prediction_prob']\n",
    "        experiment_name = csv_file.split(\"/\")[-1].split(\".\")[0]\n",
    "        \n",
    "        # 모델 정보 저장\n",
    "        models_data.append({\n",
    "            'y_true': y_true, \n",
    "            'y_prob': y_prob, \n",
    "            # 'label': experiment_name, \n",
    "            'color': sns.color_palette(\"Set2\", len(csv_list))[idx]\n",
    "        })\n",
    "    \n",
    "    # 보정 곡선 플롯\n",
    "    plot_calibration_curve(axs, models_data)\n",
    "    \n",
    "    # 레이아웃 및 출력\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage with multiple CSV files\n",
    "csv_list = [\n",
    "    # 'DUMC_analysis/Our_model.csv',\n",
    "    # 'DUMC_analysis/Our_model_external.csv',\n",
    "]\n",
    "\n",
    "plot_all(csv_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
